# üêç Tutoriel MPI en Python avec mpi4py

> S√©rie d'exercices progressifs pour apprendre la programmation parall√®le avec mpi4py

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Language: Python](https://img.shields.io/badge/Language-Python-blue.svg)](https://www.python.org/)


## Introduction

Ce git est une adaptation en Python des exercices MPI originalement √©crits en C. Python avec mpi4py offre une syntaxe plus simple et plus rapide √† √©crire tout en conservant les performances pour les calculs intensifs.

### Pourquoi Python + MPI ?

- **Syntaxe simple** : Moins de code boilerplate qu'en C
- **Productivit√©** : D√©veloppement plus rapide
- **√âcosyst√®me riche** : NumPy, SciPy, Matplotlib int√©gr√©s
- **Performances** : Comparables au C pour les calculs num√©riques

## Pr√©requis

### Connaissances requises
- Bases en Python
- Compr√©hension des listes et dictionnaires
- Notions de NumPy (facultatif mais recommand√©)

### Logiciels n√©cessaires
- Python 3.7+
- pip (gestionnaire de paquets Python)
- Une impl√©mentation MPI (Open MPI ou MPICH)

## Installation

### √âtape 1 : Installer MPI

#### Sur Ubuntu/Debian
```bash
sudo apt-get update
sudo apt-get install openmpi-bin openmpi-common libopenmpi-dev
```

#### Sur macOS
```bash
brew install open-mpi
```

#### Sur Windows (avec WSL)
```bash
wsl --install
# Puis dans WSL :
sudo apt-get install openmpi-bin openmpi-common libopenmpi-dev
```

### √âtape 2 : Installer mpi4py

```bash
# Installation avec pip
pip install mpi4py

# Ou avec conda
conda install -c conda-forge mpi4py

# V√©rifier l'installation
python -c "from mpi4py import MPI; print(MPI.Get_version())"
```

### √âtape 3 : Tester l'installation

Cr√©ez un fichier `test_mpi.py` :

```python
from mpi4py import MPI

comm = MPI.COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()

print(f"Hello from process {rank} out of {size}")
```

Ex√©cutez-le :

```bash
mpirun -np 4 python test_mpi.py
```

Si vous voyez 4 messages, tout fonctionne ! ‚úÖ

## üîÑ Diff√©rences C vs Python

| Aspect | C | Python |
|--------|---|--------|
| **Initialisation** | `MPI_Init(&argc, &argv)` | Automatique √† l'import |
| **Finalisation** | `MPI_Finalize()` | Automatique |
| **Obtenir rank** | `MPI_Comm_rank(MPI_COMM_WORLD, &rank)` | `rank = comm.Get_rank()` |
| **Send/Recv** | Pointeurs + types explicites | Objets Python natifs |
| **Tableaux** | Manipulation manuelle | NumPy arrays |
| **Compilation** | `mpicc` n√©cessaire | Interpr√©t√© |
| **Typage** | Statique | Dynamique |

### Avantages de Python
‚úÖ **Pas de gestion m√©moire manuelle**
‚úÖ **S√©rialisation automatique** des objets
‚úÖ **Syntaxe concise**
‚úÖ **Debugging plus facile**

### Inconv√©nients
‚ùå L√©g√®rement plus lent pour les petits messages
‚ùå Consommation m√©moire plus √©lev√©e

## üìö Liste des exercices

| # | Exercice | Concepts | Difficult√© |
|---|----------|----------|------------|
| 1 | Hello World parall√®le | Bases mpi4py | ‚≠ê |
| 2 | Ma√Ætre-esclave | Diff√©renciation des r√¥les | ‚≠ê |
| 3 | Premier send/recv | `send()`, `recv()` | ‚≠ê‚≠ê |
| 4 | Passage de jeton en anneau | Communications en cha√Æne | ‚≠ê‚≠ê |
| 5 | Broadcast | `bcast()` | ‚≠ê‚≠ê |
| 6 | Scatter | `scatter()` | ‚≠ê‚≠ê |
| 7 | Gather | `gather()` | ‚≠ê‚≠ê |
| 8 | Reduce | `reduce()` | ‚≠ê‚≠ê‚≠ê |
| 9 | Calcul de œÄ (Monte Carlo) | Application compl√®te | ‚≠ê‚≠ê‚≠ê‚≠ê |
| 10 | Mesure de performance | `Wtime()`, speedup | ‚≠ê‚≠ê‚≠ê |

## üìñ Exercices d√©taill√©s

### Exercice 1 : Hello World parall√®le

**Objectif** : Comprendre les concepts de base

```python
# exercice01_hello.py
from mpi4py import MPI

# Obtenir le communicateur par d√©faut
comm = MPI.COMM_WORLD

# Obtenir l'identifiant du processus (rank)
rank = comm.Get_rank()

# Obtenir le nombre total de processus (size)
size = comm.Get_size()

# Chaque processus affiche son rang
print(f"Bonjour ! Je suis le processus {rank} parmi {size}")
```

**Ex√©cution** :
```bash
mpirun -np 4 python exercice01_hello.py
```

**R√©sultat attendu** :
```
Bonjour ! Je suis le processus 0 parmi 4
Bonjour ! Je suis le processus 2 parmi 4
Bonjour ! Je suis le processus 1 parmi 4
Bonjour ! Je suis le processus 3 parmi 4
```

**Concepts cl√©s** :
- `MPI.COMM_WORLD` : Communicateur par d√©faut
- `comm.Get_rank()` : Identifiant unique (0 √† size-1)
- `comm.Get_size()` : Nombre total de processus
- **Pas besoin de Init/Finalize** : automatique !

---

### Exercice 2 : Processus ma√Ætre-esclave

**Objectif** : Diff√©rencier les r√¥les des processus

```python
# exercice02_master_slave.py
from mpi4py import MPI

comm = MPI.COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()

if rank == 0:
    # Le processus 0 est le ma√Ætre
    print(f"Je suis le MA√éTRE (processus 0)")
    print(f"J'ai {size - 1} esclaves sous mes ordres")
else:
    # Tous les autres sont des esclaves
    print(f"Je suis un esclave (processus {rank})")
```

**Ex√©cution** :
```bash
mpirun -np 4 python exercice02_master_slave.py
```

---

### Exercice 3 : Premier envoi/r√©ception

**Objectif** : Communication point-√†-point simple

```python
# exercice03_send_recv.py
from mpi4py import MPI

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

if rank == 0:
    # Processus 0 envoie
    message = 42
    print(f"Processus 0 : J'envoie {message} au processus 1")
    comm.send(message, dest=1, tag=0)
    
elif rank == 1:
    # Processus 1 re√ßoit
    message = comm.recv(source=0, tag=0)
    print(f"Processus 1 : J'ai re√ßu {message}")
```

**M√©thodes importantes** :

#### send() - Petits objets Python
```python
comm.send(obj, dest, tag=0)
# Envoie n'importe quel objet Python s√©rialisable
# Utilise pickle automatiquement
```

#### Send() - Grands tableaux NumPy
```python
import numpy as np
data = np.array([1, 2, 3])
comm.Send([data, MPI.INT], dest=1)
# Plus rapide pour les gros tableaux
```

**Types de donn√©es support√©s** :
- Entiers, floats, strings
- Listes, tuples, dictionnaires
- Objets NumPy (avec `Send`/`Recv` majuscules)
- Objets personnalis√©s (avec pickle)

---

### Exercice 4 : Passage de jeton en anneau

**Objectif** : Communications en cha√Æne

```python
# exercice04_ring.py
from mpi4py import MPI

comm = MPI.COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()

# D√©terminer les voisins dans l'anneau
next_rank = (rank + 1) % size
prev_rank = (rank - 1) % size

if rank == 0:
    # Le processus 0 initie le jeton
    jeton = 0
    print(f"Processus 0 : J'initie le jeton avec valeur {jeton}")
    comm.send(jeton, dest=next_rank)
    jeton = comm.recv(source=prev_rank)
    print(f"Processus 0 : Le jeton est revenu avec valeur {jeton}")
else:
    # Les autres processus re√ßoivent, modifient et transmettent
    jeton = comm.recv(source=prev_rank)
    print(f"Processus {rank} : J'ai re√ßu le jeton = {jeton}")
    jeton += 1
    comm.send(jeton, dest=next_rank)
    print(f"Processus {rank} : J'ai envoy√© le jeton = {jeton}")
```

**Sch√©ma** :
```
P0(0) ‚Üí P1(1) ‚Üí P2(2) ‚Üí P3(3) ‚Üí P0(4)
```

**Ex√©cution** :
```bash
mpirun -np 4 python exercice04_ring.py
```

---

### Exercice 5 : Broadcast (diffusion)

**Objectif** : Communication collective 1-vers-tous

```python
# exercice05_broadcast.py
from mpi4py import MPI

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

# Initialisation
if rank == 0:
    valeur = 777
    print(f"Processus 0 : Je diffuse la valeur {valeur}")
else:
    valeur = None

# Broadcast : TOUS les processus appellent cette fonction
valeur = comm.bcast(valeur, root=0)

# Maintenant tous ont la m√™me valeur
print(f"Processus {rank} : valeur = {valeur}")
```

**Important** :
- `bcast()` doit √™tre appel√© par **TOUS** les processus
- Le processus `root` envoie, les autres re√ßoivent
- Retourne la valeur (ne modifie pas en place)

**Avec NumPy** :
```python
import numpy as np

if rank == 0:
    data = np.array([1.0, 2.0, 3.0])
else:
    data = np.empty(3, dtype=float)

comm.Bcast(data, root=0)  # Majuscule pour NumPy
```

---

### Exercice 6 : Scatter (distribution)

**Objectif** : Distribuer des donn√©es diff√©rentes √† chaque processus

```python
# exercice06_scatter.py
from mpi4py import MPI

comm = MPI.COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()

# Pr√©paration des donn√©es (seulement sur le processus 0)
if rank == 0:
    # Cr√©er un tableau avec une valeur pour chaque processus
    data = list(range(10, 10 + size * 10, 10))  # [10, 20, 30, 40]
    print(f"Processus 0 : Je distribue {data}")
else:
    data = None

# Scatter : chaque processus re√ßoit un √©l√©ment
local_data = comm.scatter(data, root=0)

print(f"Processus {rank} : J'ai re√ßu {local_data}")
```

**R√©sultat avec 4 processus** :
```
Processus 0 : Je distribue [10, 20, 30, 40]
Processus 0 : J'ai re√ßu 10
Processus 1 : J'ai re√ßu 20
Processus 2 : J'ai re√ßu 30
Processus 3 : J'ai re√ßu 40
```

**Avec NumPy** :
```python
import numpy as np

if rank == 0:
    sendbuf = np.arange(size * 4).reshape(size, 4)
else:
    sendbuf = None

recvbuf = np.empty(4, dtype=int)
comm.Scatter(sendbuf, recvbuf, root=0)
```

---

### Exercice 7 : Gather (collecte)

**Objectif** : Rassembler les r√©sultats de tous les processus

```python
# exercice07_gather.py
from mpi4py import MPI

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

# Chaque processus calcule son carr√©
local_value = (rank + 1) ** 2
print(f"Processus {rank} : Mon carr√© est {local_value}")

# Gather : collecter tous les r√©sultats sur le processus 0
all_values = comm.gather(local_value, root=0)

if rank == 0:
    print(f"Processus 0 : Tous les carr√©s = {all_values}")
    print(f"Somme totale = {sum(all_values)}")
```

**R√©sultat avec 4 processus** :
```
Processus 0 : Mon carr√© est 1
Processus 1 : Mon carr√© est 4
Processus 2 : Mon carr√© est 9
Processus 3 : Mon carr√© est 16
Processus 0 : Tous les carr√©s = [1, 4, 9, 16]
Somme totale = 30
```

**Variantes** :
- `gather()` : collecte sur un processus
- `allgather()` : collecte distribu√©e √† tous

---

### Exercice 8 : Reduce (r√©duction)

**Objectif** : Calculer une op√©ration sur toutes les valeurs

```python
# exercice08_reduce.py
from mpi4py import MPI

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

# Chaque processus a une valeur
local_value = rank + 1
print(f"Processus {rank} : Ma valeur est {local_value}")

# R√©duction : somme de toutes les valeurs
total_sum = comm.reduce(local_value, op=MPI.SUM, root=0)

if rank == 0:
    print(f"Somme totale : {total_sum}")

# Autres op√©rations disponibles
max_value = comm.reduce(local_value, op=MPI.MAX, root=0)
min_value = comm.reduce(local_value, op=MPI.MIN, root=0)
product = comm.reduce(local_value, op=MPI.PROD, root=0)

if rank == 0:
    print(f"Maximum : {max_value}")
    print(f"Minimum : {min_value}")
    print(f"Produit : {product}")
```

**Op√©rations disponibles** :
- `MPI.SUM` : somme
- `MPI.MAX` : maximum
- `MPI.MIN` : minimum
- `MPI.PROD` : produit
- `MPI.LAND` : ET logique
- `MPI.LOR` : OU logique

**Variante allreduce** :
```python
# Tous les processus obtiennent le r√©sultat
total_sum = comm.allreduce(local_value, op=MPI.SUM)
```

---

### Exercice 9 : Calcul parall√®le de œÄ

**Objectif** : Application compl√®te avec Monte Carlo

```python
# exercice09_pi_monte_carlo.py
from mpi4py import MPI
import numpy as np

comm = MPI.COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()

# Nombre total de points
total_points = 10_000_000
points_per_process = total_points // size

# G√©n√©rer des points al√©atoires (seed diff√©rente par processus)
np.random.seed(rank)
x = np.random.random(points_per_process)
y = np.random.random(points_per_process)

# Compter les points dans le cercle (x¬≤ + y¬≤ ‚â§ 1)
inside_circle = np.sum(x**2 + y**2 <= 1.0)

print(f"Processus {rank} : {inside_circle}/{points_per_process} points dans le cercle")

# R√©duction : somme des points dans le cercle
total_inside = comm.reduce(inside_circle, op=MPI.SUM, root=0)

if rank == 0:
    # Estimation de œÄ
    pi_estimate = 4.0 * total_inside / total_points
    pi_actual = np.pi
    error = abs(pi_estimate - pi_actual)
    
    print(f"\n{'='*50}")
    print(f"Estimation de œÄ : {pi_estimate:.10f}")
    print(f"Valeur r√©elle   : {pi_actual:.10f}")
    print(f"Erreur          : {error:.10f}")
    print(f"Erreur relative : {error/pi_actual*100:.6f}%")
    print(f"{'='*50}")
```

**Ex√©cution** :
```bash
mpirun -np 4 python exercice09_pi_monte_carlo.py
```

**R√©sultat typique** :
```
Processus 0 : 1963745/2500000 points dans le cercle
Processus 1 : 1963891/2500000 points dans le cercle
Processus 2 : 1963512/2500000 points dans le cercle
Processus 3 : 1963624/2500000 points dans le cercle

==================================================
Estimation de œÄ : 3.1418704000
Valeur r√©elle   : 3.1415926536
Erreur          : 0.0002777464
Erreur relative : 0.008839%
==================================================
```

---

### Exercice 10 : Mesure de performance

**Objectif** : Comprendre le speedup et l'efficacit√©

```python
# exercice10_performance.py
from mpi4py import MPI
import numpy as np
import time

comm = MPI.COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()

# Taille du probl√®me
n = 100_000_000
local_n = n // size

# D√©but du chronom√©trage
start_time = MPI.Wtime()

# Calcul intensif : somme des carr√©s
np.random.seed(rank)
data = np.random.random(local_n)
local_sum = np.sum(data ** 2)

# Fin du calcul local
local_time = MPI.Wtime() - start_time

# R√©duction pour obtenir la somme totale
total_sum = comm.reduce(local_sum, op=MPI.SUM, root=0)

# Temps maximum (processus le plus lent)
max_time = comm.reduce(local_time, op=MPI.MAX, root=0)

if rank == 0:
    print(f"\n{'='*60}")
    print(f"R√©sultats de performance avec {size} processus")
    print(f"{'='*60}")
    print(f"Somme totale        : {total_sum:.6f}")
    print(f"Temps max           : {max_time:.6f} secondes")
    print(f"√âl√©ments par proc   : {local_n:,}")
    print(f"√âl√©ments totaux     : {n:,}")
    print(f"{'='*60}\n")

# Script de test pour diff√©rents nombres de processus
print(f"Processus {rank} : Temps = {local_time:.6f}s")
```

**Script pour tester le speedup** :

```bash
#!/bin/bash
# test_speedup.sh

echo "Test de speedup pour le calcul parall√®le"
echo "=========================================="

for np in 1 2 4 8; do
    echo ""
    echo "Avec $np processus :"
    mpirun -np $np python exercice10_performance.py
done
```

**Calcul du speedup** :

```python
# exercice10_speedup_analysis.py
from mpi4py import MPI
import numpy as np

comm = MPI.COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()

n = 50_000_000
local_n = n // size

# Mesure du temps
start = MPI.Wtime()

np.random.seed(rank)
data = np.random.random(local_n)
local_sum = np.sum(data ** 2)

total_sum = comm.reduce(local_sum, op=MPI.SUM, root=0)

end = MPI.Wtime()
time_taken = end - start

# Collecter tous les temps
all_times = comm.gather(time_taken, root=0)

if rank == 0:
    max_time = max(all_times)
    avg_time = np.mean(all_times)
    
    print(f"\nProcessus : {size}")
    print(f"Temps max : {max_time:.6f}s")
    print(f"Temps moy : {avg_time:.6f}s")
    
    # Si on a le temps s√©quentiel (√† mesurer s√©par√©ment)
    # speedup = T1 / Tn
    # efficiency = speedup / n
```

---

## üß† Concepts cl√©s mpi4py

### Communications point-√†-point

| M√©thode | Description | Usage |
|---------|-------------|-------|
| `send(obj, dest, tag)` | Envoie un objet Python | Petits objets |
| `recv(source, tag)` | Re√ßoit un objet Python | Petits objets |
| `Send(buf, dest, tag)` | Envoie un buffer NumPy | Gros tableaux |
| `Recv(buf, source, tag)` | Re√ßoit un buffer NumPy | Gros tableaux |
| `isend()` / `irecv()` | Non-bloquant | Communications asynchrones |

### Communications collectives

| M√©thode | Description | Exemple |
|---------|-------------|---------|
| `bcast(obj, root)` | Diffusion 1‚Üítous | `val = comm.bcast(val, root=0)` |
| `scatter(list, root)` | Distribution | `val = comm.scatter(data, root=0)` |
| `gather(obj, root)` | Collecte | `all = comm.gather(val, root=0)` |
| `reduce(obj, op, root)` | R√©duction | `sum = comm.reduce(val, MPI.SUM, 0)` |
| `allreduce(obj, op)` | R√©duction+diffusion | `sum = comm.allreduce(val, MPI.SUM)` |
| `allgather(obj)` | Collecte distribu√©e | `all = comm.allgather(val)` |

### Minuscule vs Majuscule

**Minuscule (pickle)** : objets Python g√©n√©riques
```python
data = {"key": "value"}
comm.send(data, dest=1)  # Utilise pickle
```

**Majuscule (buffer)** : tableaux NumPy (plus rapide)
```python
data = np.array([1, 2, 3])
comm.Send(data, dest=1)  # Acc√®s direct m√©moire
```

---

## ‚ö†Ô∏è Erreurs courantes en Python

### 1. Oublier que bcast/scatter/etc. retournent une valeur

```python
# ‚ùå FAUX
data = [1, 2, 3]
comm.bcast(data, root=0)  # data n'est pas modifi√© !

# ‚úÖ CORRECT
data = comm.bcast(data, root=0)
```

### 2. M√©langer minuscule et majuscule

```python
# ‚ùå FAUX
comm.send(np_array, dest=1)  # Lent avec pickle

# ‚úÖ CORRECT
comm.Send(np_array, dest=1)  # Rapide avec buffer
```

### 3. Utiliser la m√™me seed pour random

```python
# ‚ùå Tous g√©n√®rent les m√™mes nombres
np.random.seed(42)

# ‚úÖ Chaque processus a une seed diff√©rente
np.random.seed(rank)
# ou
np.random.seed(int(time.time()) + rank)
```

### 4. Deadlock avec send/recv

```python
# ‚ùå DEADLOCK
if rank == 0:
    data = comm.recv(source=1)  # Attend de 1
    comm.send(data, dest=1)
if rank == 1:
    data = comm.recv(source=0)  # Attend de 0
    comm.send(data, dest=0)

# ‚úÖ SOLUTION 1 : Ordre altern√©
if rank == 0:
    comm.send(data, dest=1)
    result = comm.recv(source=1)
if rank == 1:
    data = comm.recv(source=0)
    comm.send(result, dest=0)

# ‚úÖ SOLUTION 2 : Communications non-bloquantes
req1 = comm.isend(data, dest=other)
req2 = comm.irecv(source=other)
result = req2.wait()
req1.wait()
```

---

## üìä Exemple complet : Multiplication matrice-vecteur

```python
# matrix_vector_parallel.py
from mpi4py import MPI
import numpy as np

comm = MPI.COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()

# Dimensions
n = 1000  # Taille de la matrice (doit √™tre divisible par size)
local_n = n // size

# Processus 0 cr√©e les donn√©es
if rank == 0:
    A = np.random.random((n, n))
    x = np.random.random(n)
    print(f"Matrice {n}x{n} cr√©√©e")
else:
    A = None
    x = None

# Distribuer le vecteur √† tous
x = comm.bcast(x, root=0)

# Distribuer les lignes de la matrice
local_A = np.empty((local_n, n), dtype=float)
comm.Scatter(A, local_A, root=0)

# Calcul local : multiplication de local_A par x
start = MPI.Wtime()
local_result = np.dot(local_A, x)
local_time = MPI.Wtime() - start

# Collecter les r√©sultats
result = np.empty(n, dtype=float) if rank == 0 else None
comm.Gather(local_result, result, root=0)

# Afficher les temps
max_time = comm.reduce(local_time, op=MPI.MAX, root=0)

if rank == 0:
    print(f"Temps de calcul : {max_time:.6f}s")
    print(f"R√©sultat: y[0:5] = {result[:5]}")
```

---

## üìö Ressources suppl√©mentaires

### Documentation
- [mpi4py Documentation](https://mpi4py.readthedocs.io/)
- [MPI Forum](https://www.mpi-forum.org/)
- [NumPy Documentation](https://numpy.org/doc/)

### Tutoriels
- [mpi4py Tutorial officiel](https://mpi4py.readthedocs.io/en/stable/tutorial.html)
- [Python HPC avec mpi4py](https://github.com/jbornschein/mpi4py-examples)
